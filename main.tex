 
\documentclass[xcolor=svgnames]{beamer}
%
\usetheme{Singapore}
\setbeamertemplate{navigation symbols}{}
\setbeamertemplate{footline}[frame number]

%
\setbeamercolor{block body}{bg=AliceBlue}

\usepackage[all]{xy}
\usepackage{accents}
\usepackage{amsmath,amssymb}
\usepackage{array}
\usepackage{bibentry}
\usepackage{bm}
\usepackage{cancel}
\usepackage{comment}
\usepackage{etex}
\usepackage{macros}
\usepackage{mathscinet}
\usepackage{mathtools}
\usepackage{rotating}
\usepackage{tikz}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan
    }
\renewcommand{\newblock}{\relax} %% ??????????

%
%% Gianni macros
%

\DeclareMathOperator{\M}{\mathcal M}
\DeclareMathOperator{\eDeriv}{D_{\text{e}}}
\DeclareMathOperator{\grad}{grad}
\DeclareMathOperator{\mDeriv}{D_{\text{m}}}

\newcommand{\openplan}[2]{\overset{\circ}\Pi\left(#1,#2\right)}
\newcommand{\Bspaceat}[1]{B_{#1}}
\newcommand{\Bspace}[1]{B_{#1}}
\newcommand{\Ccs}[1]{C_0\left(#1\right)}
\newcommand{\Cexp}[1]{C_0^{(\cosh-1)}\left(#1\right)}
\newcommand{\Cinfcs}[1]{C_0^\infty\left(#1\right)}
\newcommand{\Cinfp}[1]{C_{\mathrm{p}}^\infty\left(#1\right)}
\newcommand{\Derivby}[1]{\frac{\Deriv}{d#1}}
\newcommand{\Lexp}[1]{L^{(\cosh-1)}\left(#1\right)}
\newcommand{\LlogL}[1]{L^{(\cosh-1)_*}\left(#1\right)}
\newcommand{\TMaxexp}{\operatorname{T}\Maxexp}
\newcommand{\WCexp}[1]{C_0^{1,(\cosh-1)}\left( #1 \right)}
\newcommand{\Wexp}[1]{W^{1,(\cosh-1)}\left(#1\right)}
\newcommand{\WlogL}[1]{W^{1,(\cosh-1)_*}\left(#1\right)}
\newcommand{\bgamma}{{\bm \gamma}}
\newcommand{\bnabla}{{\bm\nabla}}
\newcommand{\condexpectat}[3]{{\Expectation}_{#1}\left[#2 \middle| #3\right]}
\newcommand{\displacement}{\operatorname{\mathbb S}}
\newcommand{\eBspace}[1]{B_{#1}}
\newcommand{\eDerivby}[1]{\frac{\eDeriv}{d#1}}
\newcommand{\ehessianat}[2]{\prescript{e}{}\Hessian_{#1}{#2}}
\newcommand{\expbundle}{S\Maxexp}
\newcommand{\fullbundleat}[1]{\prescript{1}{}S^1\maxexpat{#1}}
\newcommand{\gaussdensity}{\gamma}
\newcommand{\gaussint}[2]{\int{#1} \gaussdensity(#2) \ d#2 \ }
\newcommand{\hullof}[1]{\operatorname{hull}\left(#1\right)}
\newcommand{\mDerivby}[1]{\frac{\mDeriv}{d#1}}
\newcommand{\maxmix}[1]{\prescript{*}{}{\Maxexp\left(#1\right)}}
\newcommand{\mhessianat}[2]{\prescript{m}{}\Hessian_{#1}{#2}}
\newcommand{\mixbundleat}[1]{\prescript{*}{}S\maxexpat{#1}}
\newcommand{\mixbundle}{\prescript{*}{}S\Maxexp}
\newcommand{\mixfiberat}[2]{{}^*S_{#1}\maxexpat{#2}}
\newcommand{\model}{\mathcal M}
\newcommand{\opensimplexat}[1]{\mathcal P_>\left(#1\right)}
\newcommand{\preBspaceat}[1]{\prescript{*}{}B_{#1}}
\newcommand{\preBspace}[1]{\prescript{*}{}B_{#1}}
\newcommand{\rosso}[1]{\textcolor{red}{#1}}
\newcommand{\sdomainat}[1]{\sdomain_{#1}}
\newcommand{\sdomain}{\mathcal S}
\newcommand{\simplexon}[1]{\mathcal P\left(#1\right)}
\newcommand{\tensorat}[3]{\prescript{#1}{}S^{#2}\maxexpat{#3}}

\renewcommand{\emph}{\rosso}
\renewcommand{\transport}[2]{{\mathbb U} _ {#1} ^ {#2}}

%
%% end Gianni's macros
%

\title{\it IG-2023: ANOVA}

\author[G Pistone]{\bf Giovanni Pistone}

\institute[CCA]{\includegraphics[height=4em]{pictures/deCastro-logo.pdf}}

\date{Revised \today}

\begin{document} 
% 
\begin{frame}\frametitle{DIMA UNIGE April 3, 2023}  

\titlepage

\tiny 
web-page: \url{www.giannidiorestino.it} 

e-mail: \url{giovanni.pistone@carloalberto.org}

orcidID: 0000-0003-2841-788X

The author acknowledges the support of de Castro Statistics, DIMA U. Genoa, INdAM-GNAMPA and DISMA, Politecnico di Torino.

\nobibliography{tutto}%

\bibliographystyle{plain}

\end{frame}

\begin{frame}[plain]\small\frametitle{Bibliography}
ANOVA Analysis of Variance is used in Statistics and Sistem Theory. It is a non-parametric  \emph{orthogonal splitting} of the vector space of $L^2$ random variable. We use it to split the fibres of the statistical bundle.

\begin{description}
\item[1968] \bibentry{hajek:1968}
    \item[1981] \bibentry{efron|stein:1981variance}
    \item[2001] \bibentry{sobol:2001global}
    \item[2021] \bibentry{pistone:2021gsi}
\end{description}
\end{frame}

    \begin{frame}[plain,allowframebreaks]\small\frametitle{ANOVA with two non-independent factors}

    \begin{itemize}
    \item Consider a finite product sample space $\Omega = \Omega_1 \times \Omega_2$ with a joint probability function 
    \begin{equation*}
       q \colon  \Omega_1 \times \Omega _2 \ni (x_1,x_2) \mapsto q(x_1,x_2) \ . 
    \end{equation*}
    We denote the two margins by 
    \begin{equation*}
    q_1(x_1) = \sum_y f(x_1,y) \ , \quad q_2(x_2) = \sum_x q(x,x_2) \ .
    \end{equation*}
    \item We focus on the case $q \neq q_1 \otimes q_2$.
    \item For each random variable $f \in L^2(q)$ we look for an orthogonal decomposition of the form
    \begin{equation*}
        f(x_1,x_2) = f_0 \oplus (f_1(x) + f_2(x_2)) \oplus f_{12}(x_1,x_2)
    \end{equation*}
    Notice that we do not require $f_1 \perp f_2$.
    \item We call \emph{factors} the two projections \begin{equation*}
    X_1(x_1,x_2) = x_1 \ , \quad X_2(x_1,x_2) = x_2 \ .  
    \end{equation*}
    Consider the subsets of $\set{1,2}$, that is $\emptyset, \set{1},\set{2},\set{1,2}$, partially ordered by inclusion. Let $X_I$ be the components projection on $I$, $X_I = (X_j \colon j \in I)$. 
    \item An \emph{effect of the factors $I$} is a random variable of the form $f_I\circ X_I$, which is $q$-orthogonal to all $g \circ X_J$ for all $J \prec I$.
    \item The order of an $I$-interaction is $\# I$. Let $H_k$ be the vector space generated by the $I$-interactions of order $k$. An effect of order 0 is a \emph{grand mean}; an effect of order 1 is a \emph{simple effect}; an effect of order 2 is an \emph{interaction}.
    \item $H_0$ contains random variables which do not depend on any $X_j$ $j=1,2$. that is, $H_0 = \reals$.
    \item The space $H_1$ is generated by the random variables of the form 
        $f_1\circ X_1$ and $f_2\circ X_2$
    with 
    \begin{gather*}
        \expectat q {f_1\circ X_1} = \expectat {q_1} {f_1} = 0 \\
           \expectat q {f_2\circ X_2} = \expectat {q_2} {f_2} = 0 
    \end{gather*}
   In fact, $q$-orthogonal to $H_0 = \reals$ is the same as zero $q$-expectation.
   \item An element of $H_1$ is of the form
   \begin{equation*}
       f_1\circ X_1 + f_2 \circ X_2 \ , \quad f_1 \in L^2_0(q_1) \ , f_2 \in L^2_0(q_2)
   \end{equation*}
   \item The representation above is unique. In fact, if
   \begin{equation*}
       f_1(x_1) + f_2(x_2) = 0 \ , x_1 \in \Omega_1, x_2 \in \Omega_2 \ ,
   \end{equation*}
   then both $f_1$ and $f_2$ must be constant. As the $q$-expectation is zero, $f_1 = f_2 = 0$.
   \item An element of $H_2$ is of the form $f_{12} \circ(X_1,X_2)$ with
   \begin{equation*}
     f_{12} \circ(X_1,X_2) \perp H_{\emptyset}, H_{\set{1}}, H_{\set{2}}  
   \end{equation*}
   The orthogonality with respect to $H_{\emptyset}$ implies zero $q$-expectation $\expectat q {f_{12}} = 0$. The orthogonality with respect to $H_{\emptyset} + H_{\set{1}}$ and $H_{\emptyset} + H_{\set{2}}$ implies zsro conditional expectation with respec to each factor:
   \begin{equation*}
       \condexpat {q} {f_{12} \circ (X_1,X_2)}{X_1} = 0 \ , \quad \condexpat {q} {f_{12} \circ (X_1,X_2)}{X_2} = 0
   \end{equation*}
    \item We look for a decomposition of $f \in L^2(q)$ of the form
    \begin{equation*}
    0 = f_0 + (f_1\circ X_1 + f_2\circ X_2) + f_{12}\circ (X_1,X_2)
    \end{equation*}
    with $f_0 \in H_0$, $(f_1\circ X_1 + f_2\circ X_2) \in H_1$, and $f_{12}\circ (X_1,X_2) \in H_2$. 
    \item Let $X_1$ be the orthogonal projection on $H_1$. This is called \emph{Hajek projection}. It is a general device to approximate a complex random variable with an additive model. 
    \item Then, the orthogonal decomposition is 
    \begin{equation*}
        f = \Expectation_q f + X_1 f + (\operatorname I  - 
 \Expectation_q - X_1) f  .
    \end{equation*}
    \item We want to compute the projection on $H_1$. It is a least square problem
    \begin{align*}
    \min& \expectat q {\avalof {f - f_0 - f_1\circ X_1 - f_2\circ X_2}^2} \\    
    &\text{s.t.} f_0 \in \reals, \expectat {q_1} {f_1} = 0, \expectat {q_2} {f_2} = 0
    \end{align*}
    \item The following equations are the gradient equations of the least square problem. They also derive from conditioning the decomposition.
    \begin{align*}
        \expectat q {f} &= f_0 \\
        \condexpat q {f}{X_1} &= f_0 + f_1\circ X_1 + \condexpat q {f_2 \circ X_2}{X_1} \\ 
            \condexpat q {f}{X_2} &= f_0 + \condexpat q {f_1 \circ X_1}{X_2} + f_2\circ X_2
        \end{align*}
        \item Assume $q =q_1 \otimes q_2$. The system of equations becomes
 \begin{align*}
        \expectat q {f} &= f_0 \\
        \condexpat q {f}{X_1} &= f_0 + f_1\circ X_1 + \cancel{\expectat {q_2} {f_2}} \\ 
            \condexpat q {f}{X_2} &= f_0 + \cancel{\expectat {q_1} {f_1}} + f_2\circ X_2 \ .
        \end{align*} 
The decomposition becomes
\begin{multline*}
f = \expectat q f + \\
\left(\condexpat q {f - \expectat q f}{X_1} + \condexpat q {f - \expectat q f}{X_2}\right) + \\ 
\left(f - \expectat q f - \condexpat q {f}{X_1} + \condexpat q {f }{X_2}\right) \ .
\end{multline*}

There is some confusion in the literature as some take the equation above as the definition of ANOVA decomposition. That equation applies to the independent case only.

\item Write $q(x_1,x_2) = k(x_1,x_2) q_i(x_1) q_2(x_2)$. The Bayes equation for conditional expectation is 
\begin{equation*}
    \condexpat q g Z = \frac{\condexpat {q_1 \otimes q_2} {kg} Z}{\condexpat {q_1 \otimes q_2} {k} Z}
\end{equation*}
\item Especially,
\begin{equation*}
   \condexpat q g {X_1=x_1} = \frac{\sum_y k(x_1,y)g(x_1,y) q_2(y)}{\sum_y k(x_1,y) q_2(y)} = \sum_y g(x_1,y) q_{2|1}(y|x_1)
\end{equation*}
and
\begin{equation*}
   \condexpat q g {X_2=x_2} = \frac{\sum_x k(x,x_2)g(x,x_2) q_1(x)}{\sum_x k(x,x_2) q_1(x)} = \sum_x g(x,x_2) q_{1|2}(x|x_2)
   \end{equation*}
   \item The equations above allow for writing a componentwise system of linear equations (see the paper).
   
        \end{itemize}


    \end{frame}

\begin{frame}[plain,allowframebreaks]\small\frametitle{Aside on Linear Programming LP}

\begin{itemize}  \item See \S IV.8 in \bibentry{barvinok:2002}
\end{itemize}

  \begin{itemize}
  \item The \emph{primal problem in canonical form} is
    \begin{align*}
      \text{Find} \quad c =& \inf \sum_x c(x) r(x) \\
      \text{Subject to} \quad &\sum_x A(y,x) r(x) = \beta(y) \quad y \in Y \\
      &r(x) \geq 0
    \end{align*}
  \item $r$ is the \emph{primal plan}
  \item a plan is \emph{feasible} if the constraints hold
  \end{itemize}

  \begin{itemize}
  \item The \emph{dual problem in standard form} is
    \begin{align*}
      \text{Find} \quad \beta =& \sup \sum_y \beta(y) \lambda(y) \\
      \text{Subject to} \quad &\sum_y A(y,x) \lambda(y) \leq c(x) 
    \end{align*}
  \item $\lambda$ is the \emph{dual plan}
  \end{itemize}

  \begin{block}{Strong duality theorem}
  If a feasible primal plan exists, then $c = \beta$. If moreover, $c > -\infty$, then primal optimal and dual optimal plans exist.  
  \end{block}
\end{frame}

\begin{frame}[plain,allowframebreaks]\small\frametitle{Fixed margins}
\begin{itemize}
    \item Assume a product sample space $X = \Omega_1 \times \Omega_2$ and consider the probability simplex $\simplexon{\Omega_1 \times \Omega_2}$. The two marginalisation mappings are
    \begin{align*}
 X_1& \colon \simplexon{\Omega_1 \times \Omega_2} \ni q(\cdot,\cdot) \mapsto \sum_{x_2} q(\cdot,x_2) \in  \simplexon{X_1}  \\
 X_2& \colon \simplexon{\Omega_1 \times \Omega_2} \ni q(\cdot,\cdot) \mapsto \sum_{x_1} q(x_1,\cdot) \in  \simplexon{X_2} 
    \end{align*}
    \item For each given $q_1 \in \simplexon{X_1}$ and $q_2 \in \simplexon{X_2}$ define 
    \begin{equation*}
        \Pi(q_1,q_2) = \setof{q \in \simplexon{\Omega_1 \times \Omega_2}} {X_1 q = q_1, X_2 q = q_2} \ .
    \end{equation*}
    \item The set of \emph{transport plans}  $\Pi(q_1,q_2)$ is \begin{itemize} \item non-empty, \item convex, \item compact.
    \end{itemize}

    \item The marginalization conditions
    \begin{gather*}
        \sum_{x_2 \in X_2} q(y_1,x_2) = q_1(y_1) \quad y_1 \in X_1 \\
            \sum_{x_1 \in X_1} q(x_1,y_2) = q_2(y_2) \quad y_2 \in X_2
    \end{gather*}
    can be written as
    \begin{gather*}
        \sum_{(x_1,x_2) \in X_1\times X_2} (y_1 = x_1) q(x_1,x_2) = q_1(y_1) \\
             \sum_{(x_1,x_2) \in X_1\times X_2} (y_2 = x_2) q(x_1,x_2) = q_2(y_2)
    \end{gather*}
    thus identifying an operator $A \colon (X_1 \cup X_2) \times (\Omega_1 \times \Omega_2)$ with
    \begin{equation*}
     \sum_{(x_1,x_2) \in \Omega_1 \times \Omega_2} A(y;x_1,x_2) q(x_1,x_2) = (q_1 \cup q_2)(y)    
    \end{equation*}
\end{itemize}    
\end{frame}

\begin{frame}[plain,allowframebreaks]\small\frametitle{Kantorovich optimal transport}
\begin{itemize}
    \item Given the \emph{cost} $c \colon \Omega_1 \times \Omega_2 \to \reals$, Kantorovich looks for the transport plan with minimal expected cost
    \begin{equation*}
        \inf \setof{\sum_{x_1,x_2} c(x_1,x_2) q(x_1,x_2)}{q \in \Pi(q_1,q_2)}
    \end{equation*}
    \item It is a primal problem in canonical form:
       \begin{align*}
      \text{Find} \quad c =& \inf \sum_{x_1,x_2} c(x_1,x_2) q(x_1,x_2) \\
      \text{Subject to} \quad &\sum_x A(y;x_1,x_2) q(x_1,x_2) = (q_1 \cup q_2)(y) \\
      &q(x_1,x_2) \geq 0 \ .
    \end{align*}
\end{itemize}

\begin{itemize}
    \item The dual problem in standard form is
    \begin{align*}
      \text{Find} \quad \beta =& \sup \sum_y (q_1 \cup q_2) (y) \lambda(y) \\
      \text{Subject to} \quad &\sum_y A(y;x_1,x_2) \lambda(y) \leq c(x_1,x_2) 
    \end{align*}
\item    that is, given the form of $A$,
\begin{align*}
      \text{Find} \quad \beta =& \sup \left(\sum_{y_1\in X_1}q_1(y_1) \lambda_1(y_1) + \sum_{y_2 \in X_2} q_2(y_2) \lambda_2(y_2)\right) \\
      \text{Subject to} \quad &\lambda_1(x_1) + \lambda_2(x_2) \leq c(x_1,x_2) \ .
    \end{align*}
\end{itemize}

\begin{itemize}
    \item There exists a feasible transport plan, and the set of plans is compact. It follows that the full strong duality theorem holds.
    \item Let $\bar q$, $\bar \lambda$ be the optimal plan and dual plan. The equality of values gives
    \begin{multline*}
    \sum_{x_1,x_2} c(x_1,x_2)\bar q(x_1,x_2) = \\ \sum_{x_1\in X_1}q_1(x_1) \bar \lambda_1(x_1) + \sum_{x_2 \in X_2} q_2(x_2) \bar \lambda_2(x_2) = \\
    \sum_{x_1,x_2} \left(\bar \lambda_1(x_1) + \bar \lambda_2(x_2)\right) \bar q(x_1,x_2) 
    \end{multline*}
    \item Now, the inequality $c \geq \lambda_1 \oplus \lambda _2$ implies
    \begin{equation*}
        c(x_1,x_2)=\bar \lambda_1(x_1) + \bar \lambda_2(x_2) \quad \text{provided $\bar q(x_1,x_2) \neq 0$}
    \end{equation*}
\end{itemize}
    
\end{frame}

\begin{frame}[plain,allowframebreaks]\small\frametitle{Transport plans in $\maxexpat X$}
 \begin{itemize}
 
     \item Define for all $q_1 \in \maxexpat{\Omega_1}$ and $q_2 \in \maxexpat{\Omega_2}$
     \begin{equation*}
         \overset{\circ}{\Pi}(q_1,q_2) = \setof{q \in \maxexpat{\Omega_1 \times \Omega_2}}{X_1 q = q_1,X_2 q= q_2}
     \end{equation*}
 
 \item  $\overset{\circ}{\Pi}$ is the set of \emph{couplings} or \emph{transport plans} of $q_1$ and $q_2$. \emph{It is a submanifold of the affine exponential manifold.
 
     \item A \emph{submanifold} of the affine manifold $(M,s_p,\eBspace p,\transport p q \colon p,q \in M)$} is a subset $N \subset M$ such that for each $q \in N$ there exists a smooth splitting of the fibre $\eBspace q = S_q N \oplus R_q N$ and the vector space $S_q N$ is the set of all velocities of curves in $N$ through $q$. 

\item The space $S_qN$ is the affine expression of the tangent space to $N$.

 \item Finite dimensional exponential families and finite-dimensional mixture models are notable examples. Notice that the orthogonal projection on the tangent space of the model provides the required splitting.

\item We consider the important example where the set $N$ is $\overset{\circ}{\Pi}$.

   
    \item The transport model will provide a second example.    

\end{itemize}
\end{frame}

\begin{frame}[plain,allowframebreaks]\small\frametitle{Velocities of $\overset{\circ}\Pi(q_1,q_2)$}

\begin{itemize}
    \item $t \mapsto q(t)$ is a smooth curve of $\maxexpat X$ with values in the set of strictly positive transport plans, $q(t) \in \overset{\circ}{\Pi}(q_1,q_2)$.
    \item For each random variable depending on the first variable only, $g_1\circ X_1$ it holds
\begin{multline*} 0 = \derivby t \expectat {q_1} {f_1} =  \derivby t \expectat {q(t)} {f_1\circ X_1} = \\ \scalarat {q(t)}  {f_1 \circ X_1 - \expectat {q(t)} {f_1\circ X_1}} {\velocity q(t)} = \expectat {q(t)} {f_1\circ X_1 \velocity q(t)} \ , \end{multline*}  Similarly on the other projection.
\item It follows that
\begin{equation*}
\boxed{\condexpectat {q(t)} {\velocity q(t)} {X_1} = 0 \quad \text{and} \quad \condexpectat {q(t)} {\velocity q(t)} {X_2} = 0}
\end{equation*}
that is, $\velocity q(t) \in H_1(q(t))$
\item Conversely, given $c \in H(q))$, the curve $t \mapsto (1+tc) \cdot q$ is defined in a neighbourhood of 0 and the velocity at 0 is $c$.
\item In conclusion, for all $q \in \overset{\circ}{\Pi}(q_1,q_2)$,
\begin{equation*}
 \boxed{S_q \overset{\circ}\Pi(q_1,q_2) = H_2(q)}
\end{equation*}
\end{itemize}
\end{frame}

\begin{frame}[plain,allowframebreaks]\small\frametitle{$\openplan{q_1}{q2}$ as an affine space}
\begin{itemize}
\item If $q , r \in \openplan {q_1} {q_2}$, given $c \in H(q)$,
\begin{equation*}
\expectat r {\left(\frac q r c\right) g_i \circ X_i} = \expectat q {c g_i \circ X_i} = 0
\end{equation*}
that is, $\frac q r c \in H(r)$.
\item $\mtransport q r$ is a co-cycle of parallel transports on the bundle
\begin{equation*}
    S \openplan {q_1}{q_2} = \setof {(q,c)}{ q \in \openplan {q_1}{q_2}, c \in H(q)} 
\end{equation*}
\item The dual transport is computed as follows. If $q, r \in \openplan {q_1}{q_2}$, $c \in S_q \openplan {q_1}{q_2} = H(q)$, and $d \in S_r \openplan {q_1}{q_2} = H(r)$, then
\begin{equation*}
\scalarat r {\mtransport q r c} d = \expectat q {cd} = \scalarat q c {d - \operatorname{Hajek}(d)}
\end{equation*}
\item Let us compute the mixture geodesic. If $(q,c) \in S \openplan {q_1}{q_2}$, an m-geodesic is a curve in $t \mapsto q(t) \in \openplan {q_1}{q_2}$ such that $(q(0),\velocity q(0)) = (q,c)$ and $\velocity q(t) = \mtransport q {q(t)} c$. It follows
\begin{equation*}
    \frac {\dot q(t)}{q(t)} = \frac q {q(t)} c \quad \text{then} \quad q(t) = (1 +tc) \cdot q
\end{equation*}
\item The m-geodesic from $q$ in the direction $c$ is $t \mapsto (1+tc)\cdot q$. The affine displacement is
\begin{equation*}
 \openplan {q_1}{q_2} \times  \openplan {q_1}{q_2} \ni (q,r) \mapsto \frac r q - 1
\end{equation*}
\item The e-geodesic from $q$ in the direction $d$ is 
\begin{equation*}
\velocity q(t) = (I - \opertorname{Hajek}(q(t)))d 
\end{equation*}

\end{itemize}

\end{frame}


\begin{frame}[plain,allowframebreaks]\small\frametitle{Gradient flow of the OT problem}

Let us discuss the \emph{Optimal Transport OT} problem in the framework of the transport model bundle. Let $c \colon \Omega_1 \times \Omega_2 \to \reals$ be a cost function and define the expected cost function
\begin{equation*}
  C \colon \maxexpat X \ni q \mapsto \expectat q c
\end{equation*}

\begin{block}{Gradient}The function $q \mapsto C(q)$ restricted to the open transport model $\overset{\circ}\Pi(q_1,q_2)$ has (statistical) gradient in $S\overset{\circ}\Pi(q_1,q_2)$ given by
\begin{equation*}
 \grad C \colon q \mapsto c_{12,q} = c - c_{0,q} - (c_{1,q} + c_{2,q}
\end{equation*}
\end{block}

% \begin{proof}
% For each smooth curve $t \mapsto \gamma(t)$, $\gamma(0) = \gamma$, we have 
% \begin{equation*}
%   \derivby t C(\gamma(t)) = \derivby t \expectat {\gamma(t)} c = \expectat {\gamma(t)} {c \velocity \gamma (t)} = \scalarat {\gamma(t)} {c_{12,\gamma(t)}} {\velocity \gamma(t)} \ .
% \end{equation*}
% where we have used the splitting at $\gamma(t)$,
% \begin{equation*}
%   c = c_{0,\gamma(t)} + (c_{1,\gamma(t)} + c_{2,\gamma(t)}) + c_{12,\gamma(t)}
% \end{equation*}
% together with the fact that the velocity $\velocity\gamma(t)$ is an interaction.\qed \end{proof}

\begin{block}{Gradient flow}The equation of the \emph{gradient flow of $C$} is
\begin{equation*}
  \velocity q = - \left(c - c_{0,q} - (c_{1,q} + c_{2,q}\right) = - c_{12,q})
\end{equation*}
\end{block}

The gradient mapping $\grad C(q)$ is defined to be the orthogonal projection of the cost $c$ onto the space of $q$-interactions $B_{12}(\gamma)$. 

\emph{Assume} $\gamma \mapsto c_{12,q}$ is defined for  all $q$. 

\begin{block}{Stationary point} If $\hat q$ is a zero of the extended gradient map, $\grad C(\hat q) = 0$, then it holds
\begin{equation*}
  c(x,y) = c_{0,q} + c_{1,q}(x) + c_{2,q}(y) \ , \quad  (x,y) \in \suppof{\hat q} \ .
\end{equation*}
\end{block}

We expect any solution $t \mapsto q(t)$ of the gradient flow equation to converge to a plan $\bar q = \lim_{t \to \infty} q(t) \in \Pi(q_1,q_2)$ such that $\expectat {\bar q} c$ is the value of the Kantorovich optimal transport problem. The form of the gradient is compatible with the classical result in OT:
%\begin{equation*}
%    \expectat {\bar\gamma} c = \min \setof{\expectat \gamma %c}{ \gamma \in \Gamma(\mu_1,\mu_2)}
%\end{equation*}

\end{frame}

  %The splitting of the statistical bundle suggests that each $\gamma \in \Delta( X_1\times X_2)$ stays at the intersection of two orthogonal manifolds, namely, the  model with the margins of $\gamma$ and the additive exponential model $\euler^{u_1+u_2 - K_\gamma(u_1 + u_2)} \cdot \gamma$, $u_1 \in B_1(\gamma)$ and $u_2 \in B_2(\gamma)$.

%   The simplest non-trivial case is the toy example with $n_1=n_2=2$. Let us use the coding $ X_1= X_2=\set{+1,-1}$. Any function $u$ on $ X$ has the pseudo-Boolean form $u(x,y)=a_0+a_1 x + a_2 y + a_{12} xy$. Notice that the monomials $1,x,y,xy$ are orthogonal in the counting measure. In particular, a probability has the form $\gamma(x,y) = \frac14(1+b_1x+b_2y+b_{12}xy)$ with margins $\mu_1(x) = \frac12(1+b_1x)$, $\mu_2(y) = \frac12(1+b_2y)$. The coefficients are the values of the moments. Given $b_1, b_2 \in ]-1,+1[$ to fix positive margins, the transport model is the 1-parameter family
% \begin{equation*} \gamma(x,y;\theta) = \frac14(1+ b_1x+ b_2y+\theta xy)\ , \quad -1 + \avalof{b_1+b_2} \leq \theta \leq 1 - \avalof{b_1 - b_2} \ . \end{equation*}
% The conditional probability functions are
% \begin{gather*} \gamma_{1|2}(x|y) = \frac12 \left(1+\frac{b_1-b_2\theta}{1-b_2^2}x+\frac{-b_1b_2 + \theta}{1-b_2^2}xy\right) \\ \gamma_{2|1}(y|x) = \frac12 \left(1+\frac{b_2-b_1\theta}{1-b_1^2}y+\frac{-b_1b_2 + \theta}{1-b_1^2}xy\right) \end{gather*}
% Let us compute the simple effects of a generic $u = \alpha_0 + \alpha_1 x + \alpha_2 y + \alpha_{12} xy$. We have $u_0 = \alpha_0 + \alpha_1 b_1 + \alpha_2 b_2 + \alpha_{12} \theta$, so that
% \begin{equation*}
%   u(x,y) - u_0 = \alpha_1 (x - b_1) + \alpha_2(y-b_2) + \alpha_{12} (xy - \theta) \ .
% \end{equation*}
% The simple effects are $u_1(x) + u_2(y) = \beta_1 (x - b_1) + \beta_2(y-b_2)$ and the orthogonality conditions $0 = \expectat \gamma {u_{12}(x - b_1)} = \expectat \gamma {u_{12}(y-b_2)}$ become
% \begin{gather*}
%   (\alpha_1-\beta_1)(1-b_1^2)+(\alpha_2-\beta_2)(\theta-b_1b_2) = - \alpha_{12}(b_2-\theta b_1) \ , \\
%   (\alpha_1-\beta_1)(\theta-b_1b_2)+(\alpha_2-\beta_2)(1 - b_2^2)= - \alpha_{12}(b_1-\theta b_2) \ . 
% \end{gather*}
% The system has a unique closed-form solutio


\end{document}

%%% Local Variables:
%%% reftex-default-bibliography: ("/home/giannidiorestino/Dropbox/InProgress/tutto.bib")
%%% End:

https://www.overleaf.com/project/64109af0a2c0904d6c5e12e8